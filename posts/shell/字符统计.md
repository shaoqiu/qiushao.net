字符统计
------

这是leetcode上的一道脚本题:[https://leetcode.com/problems/word-frequency/]

原题描述如下：
> Write a bash script to calculate the frequency of each word in a text file words.txt.
For simplicity sake, you may assume:
words.txt contains only lowercase characters and space ' ' characters.
Each word must consist of lowercase characters only.
Words are separated by one or more whitespace characters.
For example, assume that words.txt has the following content:
> ```
the day is sunny the the
the sunny is is
> ```
Your script should output the following, sorted by descending frequency:
> ```
the 4
is 3
sunny 2
day 1
> ```
Note:
Don't worry about handling ties, it is guaranteed that each word's frequency count is unique.

一看完题就想到使用`map`来实现。单词作为`key`，单词出现的次数作为`value`。刚好前面学习了`shell数组`，可以派上用场了。脚本实现如下：
```
# Read from the file words.txt and output the word frequency list to stdout.
#!/bin/bash

INPUT_FILE="words.txt"
declare -A RESULT

{
while read line
do
    for word in $line
    do
        if [ -z "${RESULT[$word]}" ]
        then
            RESULT[$word]=1
        else
            RESULT[$word]=$((${RESULT[$word]} + 1))
        fi
        #echo "$word count = ${RESULT[$word]}"
    done
done
} < $INPUT_FILE


function get_counts()
{
    for word in ${!RESULT[@]} 
    do 
        echo "$word ${RESULT[$word]}" 
    done
}

get_counts | sort -k2,2nr
```

提交后结果也是正确的。
后来想看看别人是怎么做的，一搜索，发现别人给的方法好简洁啊：
```
cat word.txt | xargs | sed 's/ /\n/g' | sort | uniq -c | sort -k1,1nr | awk '{print $2,$1}'
```

居然一行就搞定了。
不过在执行效率上，我的笨方法快了一百多毫秒。。。
因为我使用的是关联数组，也就是`HashMap`，需要做的工作就是遍历一次给出的单词，然后在`HashMap`中找到对应的单词，累计起来即可。由于`HashMap`的查找时间是常数级的。所以统计词频的时间复杂度为O(n)。统计完词频后再排序，虽然排序的时间复杂度是`O(log2(n)*n)`，但这时候排序的元素经过去重了，已经大大减少了。所以需要的时间也大大减少了。整体的时间复杂度为：`O(n) + O(log2(m)*m)`(m<=n)。
<br/>
其他人使用的方法是：
1. 排序:`cat word.txt | xargs | sed 's/ /\n/g' | sort`， 时间复杂度为`O(log2(n)*n)`
2. 统计去重:`| uniq -c `，`-c`参数为显示重复的次数，时间复杂度为`O(n)`
3. 排序：`| sort -k1,1nr`，时间复杂度为`O(log2(m)*m)`(m<=n)
整体的时间复杂度就是把上面几个步骤的时间加起来。
明显第一种方法的效率更高一些。但如果数据量小的话，还是用第二种方法更方便快捷。